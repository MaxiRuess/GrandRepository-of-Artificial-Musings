{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: '.', 1: 'b', 2: 'c', 3: 'd', 4: 'e', 5: 'f', 6: 'g', 7: 'h', 8: 'i', 9: 'j', 10: 'k', 11: 'l', 12: 'm', 13: 'n', 14: 'o', 15: 'p', 16: 'q', 17: 'r', 18: 's', 19: 't', 20: 'u', 21: 'v', 22: 'w', 23: 'x', 24: 'y', 25: 'z'}\n",
      "27\n"
     ]
    }
   ],
   "source": [
    "words = open('../../data/names.txt','r').read().splitlines()\n",
    "\n",
    "chars = sorted(list(set(''.join(words))))\n",
    "stoi = {char:i for i,char in enumerate(chars)}\n",
    "stoi['.'] = 0 \n",
    "itos = {i:char for char,i in stoi.items()}\n",
    "vocab_size = len(stoi)\n",
    "print(itos)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataset(words): \n",
    "    block_size = 3\n",
    "    X, Y = [], []\n",
    "    for w in words: \n",
    "        context = [0] * block_size\n",
    "        for c in w + '.':\n",
    "            ix = stoi[c]\n",
    "            X.append(context)\n",
    "            Y.append(ix)\n",
    "            context = context[1:] + [ix]\n",
    "            \n",
    "    X = torch.tensor(X)\n",
    "    Y = torch.tensor(Y)\n",
    "    print(X.shape, Y.shape)\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([182580, 3]) torch.Size([182580])\n",
      "torch.Size([22767, 3]) torch.Size([22767])\n",
      "torch.Size([22799, 3]) torch.Size([22799])\n"
     ]
    }
   ],
   "source": [
    "random.seed(42)\n",
    "random.shuffle(words)\n",
    "\n",
    "n1 = int(0.8 * len(words))\n",
    "n2 = int(0.9 * len(words))\n",
    "\n",
    "X_train , Y_train = build_dataset(words[:n1])\n",
    "X_val, Y_val = build_dataset(words[n1:n2])\n",
    "X_test, Y_test = build_dataset(words[n2:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare(s, dt, t): \n",
    "    ex = torch.all(dt == t.grad).item()\n",
    "    app = torch.allclose(dt, t.grad)\n",
    "    maxdiff = (dt - t.grad).abs().max().item()\n",
    "    print(f'{s:15s} | exact: {str(ex):5s} | approximate: {str(app):5s} | maxdiff: {maxdiff}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4137\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10 \n",
    "n_hidden = 64\n",
    "block_size = 3\n",
    "\n",
    "g = torch.Generator().manual_seed(42)\n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "\n",
    "# Layer 1\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1 # Don't need this becasue of batchnorm\n",
    "\n",
    "# Layer 2\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) *0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1# initalized not to zero because it unmasks potential dead neurons\n",
    "\n",
    "# Batchnorm\n",
    "bngain = torch.randn((1, n_hidden), generator=g) * 0.1 +1\n",
    "bnbias = torch.randn((1, n_hidden), generator=g) * 0.1\n",
    "\n",
    "\n",
    "params = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "print(sum(p.numel() for p in params))\n",
    "for p in params: \n",
    "    p.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "n = batch_size \n",
    "\n",
    "# Mini-batch\n",
    "ix = torch.randint(0, X_train.shape[0], (batch_size,), generator=g)\n",
    "Xb, Yb = X_train[ix], Y_train[ix]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = C[Xb] # embed the characters into vectors\n",
    "embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "# Linear layer 1\n",
    "hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "# BatchNorm layer\n",
    "bnmeani = 1/n*hprebn.sum(0, keepdim=True)\n",
    "bndiff = hprebn - bnmeani\n",
    "bndiff2 = bndiff**2\n",
    "bnvar = 1/(n-1)*(bndiff2).sum(0, keepdim=True) # note: Bessel's correction (dividing by n-1, not n)\n",
    "bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "bnraw = bndiff * bnvar_inv\n",
    "hpreact = bngain * bnraw + bnbias\n",
    "# Non-linearity\n",
    "h = torch.tanh(hpreact) # hidden layer\n",
    "# Linear layer 2\n",
    "logits = h @ W2 + b2 # output layer\n",
    "# cross entropy loss (same as F.cross_entropy(logits, Yb))\n",
    "logit_maxes = logits.max(1, keepdim=True).values\n",
    "norm_logits = logits - logit_maxes # subtract max for numerical stability\n",
    "counts = norm_logits.exp()\n",
    "counts_sum = counts.sum(1, keepdims=True)\n",
    "counts_sum_inv = counts_sum**-1 # if I use (1.0 / counts_sum) instead then I can't get backprop to be bit exact...\n",
    "probs = counts * counts_sum_inv\n",
    "logprobs = probs.log()\n",
    "loss = -logprobs[range(n), Yb].mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.4007, grad_fn=<NegBackward0>)"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for p in params:\n",
    "  p.grad = None\n",
    "for t in [logprobs, probs, counts, counts_sum, counts_sum_inv, # afaik there is no cleaner way\n",
    "          norm_logits, logit_maxes, logits, h, hpreact, bnraw,\n",
    "         bnvar_inv, bnvar, bndiff2, bndiff, hprebn, bnmeani,\n",
    "         embcat, emb]:\n",
    "  t.retain_grad()\n",
    "loss.backward()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logprobs        | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "probs           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum_inv  | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts_sum      | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "counts          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "norm_logits     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logit_maxes     | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "logits          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "h               | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b2              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hpreact         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bngain          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnbias          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnraw           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar_inv       | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnvar           | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff2         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bndiff          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "bnmeani         | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "hprebn          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "embcat          | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "W1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "b1              | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "emb             | exact: True  | approximate: True  | maxdiff: 0.0\n",
      "C               | exact: True  | approximate: True  | maxdiff: 0.0\n"
     ]
    }
   ],
   "source": [
    "dlogprobs = torch.zeros_like(logprobs)\n",
    "dlogprobs[range(n), Yb] = -1.0/n\n",
    "dprobs = (1.0 / probs) * dlogprobs\n",
    "dcounts_sum_inv = (counts * dprobs).sum(1, keepdim=True)\n",
    "dcounts = counts_sum_inv * dprobs\n",
    "dcounts_sum = (-counts_sum**-2) * dcounts_sum_inv\n",
    "dcounts += torch.ones_like(counts) * dcounts_sum\n",
    "dnorm_logits = counts * dcounts\n",
    "dlogits = dnorm_logits.clone()\n",
    "dlogit_maxes = (-dnorm_logits).sum(1, keepdim=True)\n",
    "dlogits += F.one_hot(logits.max(1).indices, num_classes=logits.shape[1]) * dlogit_maxes\n",
    "dh = dlogits @ W2.T # Need to be Matrix Multiplication and this is the only way the shapes work\n",
    "dW2 = h.T @ dlogits # Same here, there is only one way to make the shapes work\n",
    "db2 = dlogits.sum(0) \n",
    "dhpreact = (1 - h**2) * dh\n",
    "dbngain = (bnraw * dhpreact).sum(0, keepdim=True)\n",
    "dbnraw = bngain * dhpreact\n",
    "dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "dbndiff = bnvar_inv * dbnraw\n",
    "dbnvar_inv = (bndiff * dbnraw).sum(0, keepdim=True)\n",
    "dbnvar = -0.5*(bnvar + 1e-5)**-1.5 * dbnvar_inv\n",
    "dbndiff2 = (1/(n-1)) * torch.ones_like(bndiff2) * dbnvar\n",
    "dbndiff += (2 * bndiff) * dbndiff2\n",
    "dhprebn = dbndiff.clone()\n",
    "dbnmeani = (-dbndiff).sum(0, keepdim=True)\n",
    "dhprebn += 1/n * (torch.ones_like(hprebn) * dbnmeani)\n",
    "dembcat = dhprebn @ W1.T\n",
    "dW1 = embcat.T @ dhprebn\n",
    "db1 = dhprebn.sum(0)\n",
    "demb = dembcat.view(emb.shape)\n",
    "dC = torch.zeros_like(C)\n",
    "for k in range(Xb.shape[0]):\n",
    "  for j in range(Xb.shape[1]):\n",
    "    ix = Xb[k, j]\n",
    "    dC[ix] += demb[k, j]\n",
    "    \n",
    "\n",
    "compare('logprobs', dlogprobs, logprobs)\n",
    "compare('probs', dprobs, probs)\n",
    "compare('counts_sum_inv', dcounts_sum_inv, counts_sum_inv)\n",
    "compare('counts_sum', dcounts_sum, counts_sum)\n",
    "compare('counts', dcounts, counts)\n",
    "compare('norm_logits', dnorm_logits, norm_logits)\n",
    "compare('logit_maxes', dlogit_maxes, logit_maxes)\n",
    "compare('logits', dlogits, logits)\n",
    "compare('h', dh, h)\n",
    "compare('W2', dW2, W2)\n",
    "compare('b2', db2, b2)\n",
    "compare('hpreact', dhpreact, hpreact)\n",
    "compare('bngain', dbngain, bngain)\n",
    "compare('bnbias', dbnbias, bnbias)\n",
    "compare('bnraw', dbnraw, bnraw)\n",
    "compare('bnvar_inv', dbnvar_inv, bnvar_inv)\n",
    "compare('bnvar', dbnvar, bnvar)\n",
    "compare('bndiff2', dbndiff2, bndiff2)\n",
    "compare('bndiff', dbndiff, bndiff)\n",
    "compare('bnmeani', dbnmeani, bnmeani)\n",
    "compare('hprebn', dhprebn, hprebn)\n",
    "compare('embcat', dembcat, embcat)\n",
    "compare('W1', dW1, W1)\n",
    "compare('b1', db1, b1)\n",
    "compare('emb', demb, emb)\n",
    "compare('C', dC, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.40073561668396 diff 0.0\n"
     ]
    }
   ],
   "source": [
    "loss_fast = F.cross_entropy(logits, Yb)\n",
    "print(loss_fast.item(), 'diff' , (loss_fast - loss).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits          | exact: False | approximate: True  | maxdiff: 7.683411240577698e-09\n"
     ]
    }
   ],
   "source": [
    "# backward pass\n",
    "\n",
    "dlogits = F.softmax(logits, 1)\n",
    "dlogits[range(n), Yb] -= 1\n",
    "dlogits /= n\n",
    "\n",
    "compare('logits', dlogits, logits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 27]), torch.Size([32]))"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape, Yb.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0218, 0.1236, 0.0193, 0.0556, 0.0285, 0.0637, 0.0446, 0.0165, 0.0275,\n",
       "        0.0271, 0.0124, 0.0636, 0.0265, 0.0240, 0.0330, 0.0272, 0.0245, 0.0388,\n",
       "        0.0402, 0.0900, 0.0348, 0.0130, 0.0327, 0.0154, 0.0103, 0.0171, 0.0682],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(logits, 1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0218,  0.1236,  0.0193,  0.0556,  0.0285,  0.0637,  0.0446,  0.0165,\n",
       "         0.0275,  0.0271,  0.0124,  0.0636,  0.0265,  0.0240,  0.0330,  0.0272,\n",
       "         0.0245,  0.0388,  0.0402, -0.9100,  0.0348,  0.0130,  0.0327,  0.0154,\n",
       "         0.0103,  0.0171,  0.0682], grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0] * n "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9581e-09, grad_fn=<SumBackward0>)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dlogits[0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f77b86a88e0>"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(dlogits.detach(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max diff: tensor(4.7684e-07, grad_fn=<MaxBackward1>)\n"
     ]
    }
   ],
   "source": [
    "hpreact_fast = bngain * (hprebn - hprebn.mean(0, keepdim=True)) / torch.sqrt(hprebn.var(0, keepdim=True, unbiased=True) + 1e-5) + bnbias\n",
    "print('max diff:', (hpreact_fast - hpreact).abs().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hprebn          | exact: False | approximate: True  | maxdiff: 9.313225746154785e-10\n"
     ]
    }
   ],
   "source": [
    "dhprebn = bngain*bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "\n",
    "compare('hprebn', dhprebn, hprebn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0, loss: 3.40073561668396\n",
      "step: 1000, loss: 2.765106439590454\n",
      "step: 2000, loss: 2.5726866722106934\n",
      "step: 3000, loss: 2.3145179748535156\n",
      "step: 4000, loss: 2.0860652923583984\n",
      "step: 5000, loss: 2.0322234630584717\n",
      "step: 6000, loss: 2.264892816543579\n",
      "step: 7000, loss: 2.305232048034668\n",
      "step: 8000, loss: 2.5049843788146973\n",
      "step: 9000, loss: 2.223799467086792\n"
     ]
    }
   ],
   "source": [
    "n_embd = 10\n",
    "n_hidden = 64\n",
    "\n",
    "g = torch.Generator().manual_seed(42)  \n",
    "C = torch.randn((vocab_size, n_embd), generator=g)\n",
    "W1 = torch.randn((n_embd * block_size, n_hidden), generator=g) * (5/3) / ((n_embd * block_size)**0.5)\n",
    "b1 = torch.randn(n_hidden, generator=g) * 0.1\n",
    "W2 = torch.randn((n_hidden, vocab_size), generator=g) * 0.1\n",
    "b2 = torch.randn(vocab_size, generator=g) * 0.1\n",
    "bngain = torch.randn((1, n_hidden), generator=g) * 0.1 + 1\n",
    "bnbias = torch.randn((1, n_hidden), generator=g) * 0.1\n",
    "\n",
    "params = [C, W1, b1, W2, b2, bngain, bnbias]\n",
    "for p in params:\n",
    "    p.requires_grad = True\n",
    "    \n",
    "batch_size = 32\n",
    "n = batch_size\n",
    "n_steps = 10000\n",
    "\n",
    "loss_i = []\n",
    "\n",
    "for step in range(n_steps):\n",
    "    \n",
    "    # Mini-batch construction\n",
    "    ix = torch.randint(0, X_train.shape[0], (batch_size,), generator=g)\n",
    "    Xb, Yb = X_train[ix], Y_train[ix]\n",
    "    \n",
    "    emb = C[Xb] # embed the characters into vectors\n",
    "    embcat = emb.view(emb.shape[0], -1) # concatenate the vectors\n",
    "    \n",
    "    # Linear layer 1\n",
    "    hprebn = embcat @ W1 + b1 # hidden layer pre-activation\n",
    "    \n",
    "    # BatchNorm layer\n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)\n",
    "    bnvar_inv = (bnvar + 1e-5)**-0.5\n",
    "    bnraw = (hprebn - bnmean) * bnvar_inv\n",
    "    hpreact = bngain * bnraw + bnbias\n",
    "    \n",
    "    # Non-linearity\n",
    "    h = torch.tanh(hpreact) # hidden layer\n",
    "    logits = h @ W2 + b2 # output layer\n",
    "    loss = F.cross_entropy(logits, Yb)\n",
    "    \n",
    "    # backward pass\n",
    "    \n",
    "    #loss.backward() # for corrrness check\n",
    "    \n",
    "    # Manual backward pass  \n",
    "    \n",
    "    dlogits = F.softmax(logits, 1)\n",
    "    dlogits[range(n), Yb] -= 1\n",
    "    dlogits /= n\n",
    "    # Layer 2 backward\n",
    "    \n",
    "    dW2 = h.T @ dlogits\n",
    "    db2 = dlogits.sum(0)\n",
    "    dh = dlogits @ W2.T\n",
    "    \n",
    "    # Non-linearity backward\n",
    "    dhpreact = (1 - h**2) * dh\n",
    "    # BatchNorm backward\n",
    "    dbngain = (bnraw * dhpreact).sum(0, keepdim=True)  \n",
    "    dbnbias = dhpreact.sum(0, keepdim=True)\n",
    "    dhprebn = bngain * bnvar_inv/n * (n*dhpreact - dhpreact.sum(0) - n/(n-1)*bnraw*(dhpreact*bnraw).sum(0))\n",
    "    # Layer 1 backward\n",
    "    dW1 = embcat.T @ dhprebn\n",
    "    dembcat = dhprebn @ W1.T\n",
    "    db1 = dhprebn.sum(0)\n",
    "    \n",
    "    # Embedding backward\n",
    "    demb = dembcat.view(emb.shape)\n",
    "    dC = torch.zeros_like(C)\n",
    "    for k in range(Xb.shape[0]):\n",
    "        for j in range(Xb.shape[1]):\n",
    "            ix = Xb[k, j]\n",
    "            dC[ix] += demb[k, j]\n",
    "    \n",
    "    grads = [dC, dW1, db1, dW2, db2, dbngain, dbnbias]\n",
    "    \n",
    "    lf = 0.1 if step < 5000 else 0.01 \n",
    "    for p, d in zip(params, grads):\n",
    "        p.data += -lf * d\n",
    "        \n",
    "    \n",
    "    if step % 1000 == 0:\n",
    "        loss_i.append(loss.item())\n",
    "        print(f'step: {step}, loss: {loss.item()}')\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad(): \n",
    "    emb = C[X_train]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hprebn = embcat @ W1 + b1\n",
    "    \n",
    "    bnmean = hprebn.mean(0, keepdim=True)\n",
    "    bnvar = hprebn.var(0, keepdim=True, unbiased=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train loss: 2.2138736248016357\n",
      "val loss: 2.2178831100463867\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "\n",
    "def split_loss(split): \n",
    "    x, y  = {'train': (X_train, Y_train), 'val': (X_val, Y_val), 'test': (X_test, Y_test)}[split]\n",
    "    emb = C[x]\n",
    "    embcat = emb.view(emb.shape[0], -1)\n",
    "    hprebn = embcat @ W1 + b1\n",
    "    \n",
    "    hpreact = bngain * (hprebn - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "    h = torch.tanh(hpreact)\n",
    "    logits = h @ W2 + b2\n",
    "    loss = F.cross_entropy(logits, y)\n",
    "    print(f'{split} loss: {loss.item()}')\n",
    "    \n",
    "split_loss('train')\n",
    "split_loss('val')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nur.\n",
      "khni.\n",
      "kilen.\n",
      "noe.\n",
      "sinn.\n",
      "riensth.\n",
      "kesmer.\n",
      ".\n",
      "rislen.\n",
      "mi.\n",
      "k.\n",
      "nz.\n",
      "kel.\n",
      ".\n",
      "liy.\n",
      "mrosy.\n",
      "s.\n",
      "leullyinlyn.\n",
      "o.\n",
      "tiec.\n"
     ]
    }
   ],
   "source": [
    "g = torch.Generator().manual_seed(2147483647 + 10)\n",
    "\n",
    "for _ in range(20):\n",
    "    \n",
    "    out = []\n",
    "    context = [0] * block_size # initialize with all ...\n",
    "    while True:\n",
    "      # ------------\n",
    "      # forward pass:\n",
    "      # Embedding\n",
    "      emb = C[torch.tensor([context])] # (1,block_size,d)      \n",
    "      embcat = emb.view(emb.shape[0], -1) # concat into (N, block_size * n_embd)\n",
    "      hpreact = embcat @ W1 + b1\n",
    "      hpreact = bngain * (hpreact - bnmean) * (bnvar + 1e-5)**-0.5 + bnbias\n",
    "      h = torch.tanh(hpreact) # (N, n_hidden)\n",
    "      logits = h @ W2 + b2 # (N, vocab_size)\n",
    "      # ------------\n",
    "      # Sample\n",
    "      probs = F.softmax(logits, dim=1)\n",
    "      ix = torch.multinomial(probs, num_samples=1, generator=g).item()\n",
    "      context = context[1:] + [ix]\n",
    "      out.append(ix)\n",
    "      if ix == 0:\n",
    "        break\n",
    "    \n",
    "    print(''.join(itos[i] for i in out))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
