{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "from collections import Counter\n",
    "from torchvision import transforms, datasets\n",
    "import torchvision\n",
    "import torchmetrics\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import torchinfo\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import CSVLogger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "NUM_EPOCHS = 5\n",
    "LEARNING_RATE = 0.01\n",
    "NUM_WORKERS = 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataModule(pl.LightningDataModule): \n",
    "    def __init__(self, data_path = '../../data'): \n",
    "        super().__init__()\n",
    "        self.data_path = data_path\n",
    "        \n",
    "    def prepare_data(self):\n",
    "        \n",
    "        datasets.MNIST(root = self.data_path, \n",
    "                       download = True)\n",
    "        \n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((32, 32)),\n",
    "            transforms.ToTensor(),\n",
    "            ])\n",
    "        \n",
    "    def setup(self, stage = None):\n",
    "        \n",
    "        train = datasets.MNIST(root = self.data_path, \n",
    "                                train = True, \n",
    "                                transform = self.transform, \n",
    "                                download = False)\n",
    "        \n",
    "        self.test = datasets.MNIST(root = self.data_path, \n",
    "                                   train = False, \n",
    "                                   transform = self.transform, \n",
    "                                   download = False)\n",
    "        \n",
    "        self.train, self.val = random_split(train, [55000, 5000])\n",
    "        \n",
    "    \n",
    "    def train_dataloader(self): \n",
    "        \n",
    "        train_loader = DataLoader(self.train,\n",
    "                                    batch_size = BATCH_SIZE,\n",
    "                                    num_workers = NUM_WORKERS,\n",
    "                                    shuffle = True, \n",
    "                                    persistent_workers=True)\n",
    "        return train_loader\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "            \n",
    "            val_loader = DataLoader(self.val,\n",
    "                                    batch_size = BATCH_SIZE,\n",
    "                                    num_workers = NUM_WORKERS,\n",
    "                                    shuffle = False, \n",
    "                                    persistent_workers=True)\n",
    "            return val_loader\n",
    "    \n",
    "    def test_dataloader(self):\n",
    "            \n",
    "            test_loader = DataLoader(self.test,\n",
    "                                    batch_size = BATCH_SIZE,\n",
    "                                    num_workers = NUM_WORKERS,\n",
    "                                    shuffle = False, \n",
    "                                    persistent_workers=True)\n",
    "            return test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PytorchLeNet(nn.Module): \n",
    "    def __init__(self, num_classes, grayscale = False):\n",
    "        super(PytorchLeNet, self).__init__()\n",
    "        \n",
    "        self.grayscale = grayscale\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        if self.grayscale:\n",
    "            in_channels = 1\n",
    "        else:\n",
    "            in_channels = 3\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 6, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size = 2),\n",
    "            nn.Conv2d(6, 16, kernel_size = 5),\n",
    "            nn.Tanh(),\n",
    "            nn.MaxPool2d(kernel_size = 2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(16*5*5, 120),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(84, num_classes)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==============================================================================================\n",
       "Layer (type:depth-idx)                   Input Shape        Output Shape       Param #\n",
       "==============================================================================================\n",
       "PytorchLeNet                             [32, 1, 32, 32]    [32, 10]           --\n",
       "├─Sequential: 1-1                        [32, 1, 32, 32]    [32, 16, 5, 5]     --\n",
       "│    └─Conv2d: 2-1                       [32, 1, 32, 32]    [32, 6, 28, 28]    156\n",
       "│    └─Tanh: 2-2                         [32, 6, 28, 28]    [32, 6, 28, 28]    --\n",
       "│    └─MaxPool2d: 2-3                    [32, 6, 28, 28]    [32, 6, 14, 14]    --\n",
       "│    └─Conv2d: 2-4                       [32, 6, 14, 14]    [32, 16, 10, 10]   2,416\n",
       "│    └─Tanh: 2-5                         [32, 16, 10, 10]   [32, 16, 10, 10]   --\n",
       "│    └─MaxPool2d: 2-6                    [32, 16, 10, 10]   [32, 16, 5, 5]     --\n",
       "├─Sequential: 1-2                        [32, 400]          [32, 10]           --\n",
       "│    └─Linear: 2-7                       [32, 400]          [32, 120]          48,120\n",
       "│    └─Tanh: 2-8                         [32, 120]          [32, 120]          --\n",
       "│    └─Linear: 2-9                       [32, 120]          [32, 84]           10,164\n",
       "│    └─Tanh: 2-10                        [32, 84]           [32, 84]           --\n",
       "│    └─Linear: 2-11                      [32, 84]           [32, 10]           850\n",
       "==============================================================================================\n",
       "Total params: 61,706\n",
       "Trainable params: 61,706\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 13.54\n",
       "==============================================================================================\n",
       "Input size (MB): 0.13\n",
       "Forward/backward pass size (MB): 1.67\n",
       "Params size (MB): 0.25\n",
       "Estimated Total Size (MB): 2.05\n",
       "=============================================================================================="
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PytorchLeNet(num_classes=10, grayscale=True)\n",
    "summary = torchinfo.summary(model, (32, 1, 32, 32), \n",
    "                            col_names=(\"input_size\", \"output_size\", \"num_params\"),\n",
    "                            col_width=18)\n",
    "\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LightningModel(pl.LightningModule): \n",
    "    def __init__(self, model, learning_rate): \n",
    "        super(LightningModel, self).__init__()\n",
    "        self.model = model\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        self.save_hyperparameters(ignore=['model'])\n",
    "        \n",
    "        \n",
    "        self.train_acc = torchmetrics.Accuracy(task= 'multiclass', num_classes = 10)\n",
    "        self.val_acc = torchmetrics.Accuracy(task= 'multiclass', num_classes = 10)\n",
    "        self.test_acc = torchmetrics.Accuracy(task= 'multiclass', num_classes = 10)\n",
    "        \n",
    "    def shared_step (self, batch):\n",
    "        x, y = batch\n",
    "        logits = self.model(x)\n",
    "        loss = torch.nn.functional.cross_entropy(logits, y)\n",
    "        return loss, y, logits\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, y, logits = self.shared_step(batch)\n",
    "        self.train_acc(logits, y)\n",
    "        self.log('train_loss', loss)\n",
    "        \n",
    "        self.model.eval()\n",
    "        with torch.inference_mode():\n",
    "            _, y, logits = self.shared_step(batch)\n",
    "            self.train_acc(logits, y)\n",
    "            \n",
    "            self.log('train_acc', self.train_acc, on_step = False, on_epoch = True)\n",
    "        return loss\n",
    "    \n",
    "    def testing_step(self, batch, batch_idx):\n",
    "        loss, y, logits = self.shared_step(batch)\n",
    "        self.test_acc(logits, y)\n",
    "        self.log('test_loss', loss)\n",
    "        self.log('test_acc', self.test_acc, on_step = False, on_epoch = True)\n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, y, logits = self.shared_step(batch)\n",
    "        self.val_acc(logits, y)\n",
    "        self.log('val_loss', loss)\n",
    "        self.log('val_acc', self.val_acc, on_step = False, on_epoch = True)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr = self.learning_rate)\n",
    "        return optimizer\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "data_module = DataModule(data_path='../../data')\n",
    "model = PytorchLeNet(num_classes=10, grayscale=True)\n",
    "\n",
    "lightning_model = LightningModel(model, LEARNING_RATE)\n",
    "\n",
    "callbacks = [ModelCheckpoint(monitor='val_acc', mode='max', save_top_k=1)]\n",
    "logger = CSVLogger('logs', name='LeNet_MNIST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "Missing logger folder: logs/LeNet_MNIST\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | PytorchLeNet       | 61.7 K\n",
      "1 | train_acc | MulticlassAccuracy | 0     \n",
      "2 | val_acc   | MulticlassAccuracy | 0     \n",
      "3 | test_acc  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "61.7 K    Trainable params\n",
      "0         Non-trainable params\n",
      "61.7 K    Total params\n",
      "0.247     Total estimated model params size (MB)\n",
      "python(78751) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(78752) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78808) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78809) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78810) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78811) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78812) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78813) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78814) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78815) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78816) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78817) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(78873) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78874) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78875) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78876) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78877) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78878) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78879) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78880) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78881) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(78883) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   0%|          | 0/215 [00:00<?, ?it/s] "
     ]
    }
   ],
   "source": [
    "\n",
    "import time \n",
    "start = time.time()\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=NUM_EPOCHS,\n",
    "                     callbacks=callbacks,\n",
    "                        logger=logger,\n",
    "                        devices='auto', \n",
    "                        accelerator='auto', \n",
    "                        log_every_n_steps=100)\n",
    "\n",
    "trainer.fit(lightning_model, data_module)\n",
    "    \n",
    "end = time.time()   \n",
    "\n",
    "print(f\"Training time: {end-start}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
