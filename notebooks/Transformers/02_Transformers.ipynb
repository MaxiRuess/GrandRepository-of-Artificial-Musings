{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math \n",
    "import copy \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiheadAttention(nn.Module): \n",
    "    def __init__(self, num_heads, d_model): \n",
    "        super(MultiheadAttention, self).__init__()\n",
    "        assert d_model % num_heads == 0 # d_model must be divisible by the number of heads so the dimension can be split evenly\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model # dimension of the model\n",
    "        self.d_k = d_model // num_heads # dimension of the key, query, and value. Each head will operate on a d_k dimensional space but the total dimension of the model is d_model\n",
    "        \n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None): \n",
    "        # Matrix multiplication of Q and K which is then scaled by the square root of the dimensionality of the key vectors (d_k)\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        \n",
    "        if mask is not None: \n",
    "            scores = scores.masked_fill(mask == 0, -1e9) # Set the scores to -inf where the mask is 0 so that the softmax will ignore these values\n",
    "            \n",
    "        # Compute the attention weights\n",
    "        attention = torch.softmax(scores, dim=-1)\n",
    "        \n",
    "        # Apply the attention weights to the value vectors to get the weighted sum\n",
    "        return torch.matmul(attention, V)\n",
    "    \n",
    "    def split_heads(self, x): \n",
    "        \n",
    "        batch_size, seq_len, d_model = x.size()\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.d_k).transpose(1, 2) # Split the last dimension into num_heads and transpose the dimensions to facilitate the parallel computation of the attention heads\n",
    "        return x\n",
    "    \n",
    "    def combine_heads(self, x): \n",
    "        # After the attention heads have been computed, the results are concatenated and passed through a linear layer to get the final output\n",
    "        batch_size, num_heads, seq_len, d_k = x.size()\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, seq_len, self.d_model)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        # Foward pass of the multihead attention layer, applies linear transformations to the input Q, K, and V to get the query, key, and value vectors\n",
    "        Q = self.split_heads(self.W_q(Q))\n",
    "        K = self.split_heads(self.W_k(K))\n",
    "        V = self.split_heads(self.W_v(V))\n",
    "        # Compute the attention heads\n",
    "        x = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        # Results are combined and passed through a linear layer to get the final output\n",
    "        x = self.combine_heads(x)\n",
    "        return self.W_o(x)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionWiseFeedForward(nn.Module): \n",
    "    def __init__(self, d_model, d_ff):\n",
    "        # Dimension of the model and the dimension of the feed forward layer \n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.fc1 = nn.Linear(d_model, d_ff)\n",
    "        self.fc2= nn.Linear(d_ff, d_model)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): \n",
    "    def __init__(self, d_model, max_len): \n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        pe = torch.zeros(size=(max_len, d_model)) # Initialize the position encoding matrix\n",
    "    \n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # Div term is used to scale the position encoding values, it is the denominator of the exponent in the sin and cos functions below. \n",
    "        # This uses a geometric progression to ensure that the model can generalize to sequences of different lengths\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) \n",
    "        \n",
    "        # This allows the model to learn to attend to the position of the tokens in the sequence\n",
    "        pe[:, 0::2] = torch.sin(position * div_term) # Compute the sine values for the even indices\n",
    "        pe[:, 1::2] = torch.cos(position * div_term) # Compute the cosine values for the odd indices\n",
    "        \n",
    "        # Register the position encoding as a buffer so that it is saved as part of the model state\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    \n",
    "    def forward(self, x): \n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bringing both together to create the encoder layer\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attention = MultiheadAttention(num_heads, d_model)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        # Layer normalization is applied to the output of the multihead attention and feed forward layers\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        # Apply the multihead attention layer and add the residual connection\n",
    "        x = self.norm1(x + self.dropout(self.self_attention(x, x, x, mask)))\n",
    "        # Apply the feed forward layer and add the residual connection\n",
    "        x = self.norm2(x + self.dropout(self.feed_forward(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Decoder Layer\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attention = MultiheadAttention(num_heads, d_model)\n",
    "        self.cross_attention = MultiheadAttention(num_heads, d_model)\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, encoding_output, src_mask, tgt_mask):\n",
    "        # Both masks a later generated in the transformer class\n",
    "        # tgt_mask is used to prevent the decoder from attending to future tokens maintaining the autoregressive property\n",
    "        attention_output = self.self_attention(x, x, x, tgt_mask) \n",
    "        x = self.norm1(x + self.dropout(attention_output))\n",
    "        # src_mask is used to prevent the decoder from attending to padding tokens\n",
    "        attention_output = self.cross_attention(x, encoding_output, encoding_output, src_mask) \n",
    "        x = self.norm2(x + self.dropout(attention_output))\n",
    "        ff_output = self.feed_forward(x)\n",
    "        x = self.norm3(x + self.dropout(ff_output))\n",
    "        return x "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the Transformer\n",
    "class Transformer(nn.Module): \n",
    "    def __init__(self, num_encoder_layers, num_decoder_layers, d_model, num_heads, d_ff, src_vocab_size, tgt_vocab_size, max_len, dropout=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        \n",
    "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model) # Embedding layer for the encoder with the source vocabulary size and the dimension of the model\n",
    "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model) # Embedding layer for the decoder with the target vocabulary size and the dimension of the model\n",
    "        self.positional_encoding = PositionalEncoding(d_model, max_len) # Positional encoding layer\n",
    "        \n",
    "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_encoder_layers)]) # List of encoder layers\n",
    "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_decoder_layers)]) # List of decoder layers\n",
    "        \n",
    "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def generate_mask(self, source, target): \n",
    "        # Generate the source and target masks\n",
    "        source_mask = (source != 0).unsqueeze(1).unsqueeze(2) # Create a mask to prevent the encoder from attending to padding tokens + unsqueeze to add the head dimension\n",
    "        target_mask = (target != 0).unsqueeze(1).unsqueeze(2) # Create mask to allow the decoder to only attend to tokens that have been generated so far + unsqueeze to add the head dimension\n",
    "        \n",
    "        seq_len = target.size(1)\n",
    "        # Create a mask to prevent the decoder from attending to future tokens to maintain the autoregressive property\n",
    "        nopeak_mask = torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1).bool()\n",
    "        target_mask = target_mask & nopeak_mask\n",
    "        \n",
    "        return source_mask, target_mask\n",
    "    \n",
    "    def forward(self, source, target):\n",
    "    \n",
    "        source_mask, target_mask = self.generate_mask(source, target)\n",
    "        \n",
    "        source_embedding = self.dropout(self.positional_encoding(self.encoder_embedding(source)))\n",
    "        target_embedding = self.dropout(self.positional_encoding(self.decoder_embedding(target)))\n",
    "        \n",
    "        encoding_output = source_embedding\n",
    "        for enc_layer in self.encoder_layers: \n",
    "            encoding_output = enc_layer(encoding_output, source_mask)\n",
    "            \n",
    "        decoding_output = target_embedding\n",
    "        for dec_layer in self.decoder_layers: \n",
    "            decoding_output = dec_layer(decoding_output, encoding_output, source_mask, target_mask)\n",
    "            \n",
    "        return self.fc(decoding_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab_size = 5000\n",
    "tgt_vocab_size = 5000\n",
    "d_model = 512\n",
    "num_heads = 8\n",
    "num_encoder_layers = 6\n",
    "num_decoder_layers = 6\n",
    "\n",
    "d_ff = 2048\n",
    "max_len = 100\n",
    "dropout = 0.1\n",
    "\n",
    "transformer = Transformer(src_vocab_size=src_vocab_size, tgt_vocab_size=tgt_vocab_size, d_model=d_model, num_heads=num_heads, \n",
    "                          num_encoder_layers=num_encoder_layers, num_decoder_layers=num_decoder_layers, d_ff=d_ff, max_len=max_len, dropout=dropout)\n",
    "\n",
    "src_data = torch.randint(1, src_vocab_size, (64, max_len))  # (batch_size, seq_length)\n",
    "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_len))  # (batch_size, seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 8.069128036499023\n",
      "Epoch [2], Loss: 8.157293319702148\n",
      "Epoch [3], Loss: 8.14328670501709\n",
      "Epoch [4], Loss: 8.11997127532959\n",
      "Epoch [5], Loss: 8.09232234954834\n",
      "Epoch [6], Loss: 8.062867164611816\n",
      "Epoch [7], Loss: 7.974868297576904\n",
      "Epoch [8], Loss: 7.8291120529174805\n",
      "Epoch [9], Loss: 7.776881217956543\n",
      "Epoch [10], Loss: 7.848037242889404\n",
      "Epoch [11], Loss: 7.6584553718566895\n",
      "Epoch [12], Loss: 7.199096202850342\n",
      "Epoch [13], Loss: 6.92490816116333\n",
      "Epoch [14], Loss: 7.036306858062744\n",
      "Epoch [15], Loss: 6.701191425323486\n",
      "Epoch [16], Loss: 6.306272983551025\n",
      "Epoch [17], Loss: 6.134025573730469\n",
      "Epoch [18], Loss: 5.979645729064941\n",
      "Epoch [19], Loss: 5.762683391571045\n",
      "Epoch [20], Loss: 5.278907775878906\n",
      "Epoch [21], Loss: 5.5385260581970215\n",
      "Epoch [22], Loss: 5.178043842315674\n",
      "Epoch [23], Loss: 5.00003719329834\n",
      "Epoch [24], Loss: 5.025485515594482\n",
      "Epoch [25], Loss: 4.30435037612915\n",
      "Epoch [26], Loss: 3.8983993530273438\n",
      "Epoch [27], Loss: 3.771472692489624\n",
      "Epoch [28], Loss: 4.011817932128906\n",
      "Epoch [29], Loss: 3.6462042331695557\n",
      "Epoch [30], Loss: 3.0175974369049072\n",
      "Epoch [31], Loss: 2.7668445110321045\n",
      "Epoch [32], Loss: 2.4377171993255615\n",
      "Epoch [33], Loss: 2.210592269897461\n",
      "Epoch [34], Loss: 1.921255350112915\n",
      "Epoch [35], Loss: 1.6800694465637207\n",
      "Epoch [36], Loss: 1.52946937084198\n",
      "Epoch [37], Loss: 1.356273889541626\n",
      "Epoch [38], Loss: 1.156383991241455\n",
      "Epoch [39], Loss: 0.9347515106201172\n",
      "Epoch [40], Loss: 0.7916700839996338\n",
      "Epoch [41], Loss: 0.6765362024307251\n",
      "Epoch [42], Loss: 0.5369144678115845\n",
      "Epoch [43], Loss: 0.4620595872402191\n",
      "Epoch [44], Loss: 0.3906323313713074\n",
      "Epoch [45], Loss: 0.31677111983299255\n",
      "Epoch [46], Loss: 0.26859766244888306\n",
      "Epoch [47], Loss: 0.2233438342809677\n",
      "Epoch [48], Loss: 0.18776386976242065\n",
      "Epoch [49], Loss: 0.1588214933872223\n",
      "Epoch [50], Loss: 0.13603520393371582\n",
      "Epoch [51], Loss: 0.11632867902517319\n",
      "Epoch [52], Loss: 0.10144805163145065\n",
      "Epoch [53], Loss: 0.08937369287014008\n",
      "Epoch [54], Loss: 0.07825684547424316\n",
      "Epoch [55], Loss: 0.06982552260160446\n",
      "Epoch [56], Loss: 0.06220143288373947\n",
      "Epoch [57], Loss: 0.05640769749879837\n",
      "Epoch [58], Loss: 0.050936732441186905\n",
      "Epoch [59], Loss: 0.04661558195948601\n",
      "Epoch [60], Loss: 0.04230978339910507\n",
      "Epoch [61], Loss: 0.03900988772511482\n",
      "Epoch [62], Loss: 0.03554670140147209\n",
      "Epoch [63], Loss: 0.033536892384290695\n",
      "Epoch [64], Loss: 0.031108837574720383\n",
      "Epoch [65], Loss: 0.02873048558831215\n",
      "Epoch [66], Loss: 0.026772644370794296\n",
      "Epoch [67], Loss: 0.02529718168079853\n",
      "Epoch [68], Loss: 0.02368801273405552\n",
      "Epoch [69], Loss: 0.02259785868227482\n",
      "Epoch [70], Loss: 0.0215559471398592\n",
      "Epoch [71], Loss: 0.020221101120114326\n",
      "Epoch [72], Loss: 0.019443834200501442\n",
      "Epoch [73], Loss: 0.018488194793462753\n",
      "Epoch [74], Loss: 0.01765250973403454\n",
      "Epoch [75], Loss: 0.017085812985897064\n",
      "Epoch [76], Loss: 0.016345355659723282\n",
      "Epoch [77], Loss: 0.015626203268766403\n",
      "Epoch [78], Loss: 0.015298666432499886\n",
      "Epoch [79], Loss: 0.014505705796182156\n",
      "Epoch [80], Loss: 0.013984572142362595\n",
      "Epoch [81], Loss: 0.013581950217485428\n",
      "Epoch [82], Loss: 0.013163291849195957\n",
      "Epoch [83], Loss: 0.012878512032330036\n",
      "Epoch [84], Loss: 0.012479986064136028\n",
      "Epoch [85], Loss: 0.012170692905783653\n",
      "Epoch [86], Loss: 0.011880999431014061\n",
      "Epoch [87], Loss: 0.011539089493453503\n",
      "Epoch [88], Loss: 0.011253303848206997\n",
      "Epoch [89], Loss: 0.01089825201779604\n",
      "Epoch [90], Loss: 0.010743782855570316\n",
      "Epoch [91], Loss: 0.010514099150896072\n",
      "Epoch [92], Loss: 0.010308823548257351\n",
      "Epoch [93], Loss: 0.010033201426267624\n",
      "Epoch [94], Loss: 0.009854581207036972\n",
      "Epoch [95], Loss: 0.009666785597801208\n",
      "Epoch [96], Loss: 0.009751142002642155\n",
      "Epoch [97], Loss: 0.009414534084498882\n",
      "Epoch [98], Loss: 0.009147141128778458\n",
      "Epoch [99], Loss: 0.009138700552284718\n",
      "Epoch [100], Loss: 0.008979021571576595\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "optimizer = optim.Adam(transformer.parameters(), lr=0.001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "transformer.train()\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    output = transformer(src_data, tgt_data[:, :-1])\n",
    "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:, 1:].contiguous().view(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch [{epoch +1 }], Loss: {loss.item()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
