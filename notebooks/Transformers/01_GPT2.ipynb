{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import math \n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_available(): device = \"mps\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputEmbedding(nn.Module):\n",
    "    def __init__(self, emb_dim, vocab_size):\n",
    "        super(InputEmbedding, self).__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.emb_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded_x = self.embedding(x)\n",
    "        scaled_embedded = embedded_x * torch.sqrt(torch.tensor(self.emb_dim).float())\n",
    "        \n",
    "        return scaled_embedded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module): \n",
    "    def __init__(self, embeded_dim, max_sequence_lenght, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        self.embeded_dim = embeded_dim\n",
    "        self.max_sequence_lenght = max_sequence_lenght\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.positional_encoding = self.compute_positional_encoding(max_sequence_lenght, embeded_dim)\n",
    "        \n",
    "    def compute_positional_encoding(self, max_sequence_lenght, embeded_dim):\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            positional_encoding = torch.zeros(max_sequence_lenght, embeded_dim)\n",
    "            position = torch.arange(0, max_sequence_lenght, dtype=torch.float).unsqueeze(1)\n",
    "            \n",
    "            div_term = torch.exp(torch.arange(0, embeded_dim, 2).float() * -(math.log(10000.0) / embeded_dim))\n",
    "            positional_encoding[:, 0::2] = torch.sin(position.float() * div_term)\n",
    "            positional_encoding[:, 1::2] = torch.cos(position.float() * div_term)\n",
    "            \n",
    "            positional_encoding = positional_encoding.unsqueeze(0)\n",
    "            \n",
    "        return positional_encoding\n",
    "    \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.positional_encoding[:, :x.size(1)].to(x.device)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    def __init__(self, embedded_dim, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(embedded_dim).uniform_())\n",
    "        self.b_2 = nn.Parameter(torch.zeros(embedded_dim).uniform_())\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForwardBlock(nn.Module):\n",
    "    def __init__(self, embedded_dim, feed_forward_dim, dropout=0.1):\n",
    "        super(FeedForwardBlock, self).__init__()\n",
    "        \n",
    "        self.embedded_dim = embedded_dim\n",
    "        self.feed_forward_dim = feed_forward_dim\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "        self.linear1 = nn.Linear(self.embedded_dim, self.feed_forward_dim)\n",
    "        self.linear2 = nn.Linear(self.feed_forward_dim, self.embedded_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_square_subsequent_mask(self, sz, device = 'mps'):\n",
    "    mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module): \n",
    "    def __init__(self, embed_dim, num_heads, attention_dropout=0.1, ff_dropout=0.1, max_len=512):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert self.head_dim * num_heads == self.embed_dim, \"Embedding dimension must be divisible by the number of heads\"\n",
    "        \n",
    "        self.query = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.key = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        self.value = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "        self.attention_dropout = nn.Dropout(p=attention_dropout)\n",
    "        self.out_dropout = nn.Dropout(p=ff_dropout)\n",
    "        \n",
    "        self.out = nn.Linear(self.embed_dim, self.embed_dim)\n",
    "        \n",
    "        self.register_buffer(\n",
    "                        \"mask\",\n",
    "                            torch.triu(torch.ones(max_len, max_len, dtype=torch.bool), diagonal=1)\n",
    "            )\n",
    "        \n",
    "    def forward(self, x, mask= None): \n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        \n",
    "        query = self.query(x).view(batch_size, seq_len, self.num_heads, self.head_dim, -1).transpose(1, 2)\n",
    "        key = self.key(x).view(batch_size, seq_len, self.num_heads, self.head_dim, -1).transpose(1, 2)\n",
    "        value = self.value(x).view(batch_size, seq_len, self.num_heads, self.head_dim, -1).transpose(1, 2)\n",
    "        \n",
    "        attention = torch.einsum(\"b h i d, b h j d -> b h i j\", query, key) / math.sqrt(self.head_dim)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask==0 , float('-inf'))\n",
    "            \n",
    "        attention = self.attention_dropout(F.softmax(attention, dim=-1))\n",
    "        \n",
    "        y = torch.einsum(\"b h i j, b h j d -> b h i d\", attention, value).transpose(1, 2).reshape(batch_size, seq_len, self.embed_dim)\n",
    "        \n",
    "        return self.out(self.out_dropout(y))\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnection(nn.Module): \n",
    "    def __init__(self, embedded_dim, dropout=0.1):\n",
    "        super(ResidualConnection, self).__init__()\n",
    "        \n",
    "        self.layer_norm = LayerNorm(embedded_dim)\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        \n",
    "    def forward(self, x, sublayer):\n",
    "        \n",
    "        norm_x = self.layer_norm(x)\n",
    "        sublayer_x = sublayer(norm_x)\n",
    "        x = x + self.dropout(sublayer_x)\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProjectionHead(nn.Module): \n",
    "    def __init__(self, embedded_dim, vocab_size):\n",
    "        super(ProjectionHead, self).__init__()\n",
    "        \n",
    "        self.linear = nn.Linear(embedded_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    def __init__(self, embedded_dim, num_heads, feed_forward_dim, attention_dropout=0.1, ff_dropout=0.1, max_len=512):\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(embedded_dim, num_heads, attention_dropout, ff_dropout, max_len)\n",
    "        self.residual_connection1 = ResidualConnection(embedded_dim)\n",
    "        \n",
    "        self.feed_forward_block = FeedForwardBlock(embedded_dim, feed_forward_dim, ff_dropout)\n",
    "        self.residual_connection2 = ResidualConnection(embedded_dim)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        x = self.residual_connection1(x, lambda x: self.multi_head_attention(x, mask))\n",
    "        x = self.residual_connection2(x, self.feed_forward_block)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module): \n",
    "    def __init__(self, vocab_size, \n",
    "                 embedded_dim, \n",
    "                 max_len, \n",
    "                 embedding_dropout=0.1, \n",
    "                 num_blocks = 6,  \n",
    "                 num_heads=8, \n",
    "                 feed_forward_dim=2048, \n",
    "                 attention_dropout=0.1,\n",
    "                    ff_dropout=0.1):\n",
    "        super(GPT, self).__init__()\n",
    "        self.max_len = max_len\n",
    "        self.token_embedding = InputEmbedding(embedded_dim, vocab_size)\n",
    "        self.positional_encoding = PositionalEncoding(embedded_dim, max_len, embedding_dropout)\n",
    "        \n",
    "        self.blocks = nn.ModuleList([DecoderBlock(embedded_dim, \n",
    "                                                  num_heads, \n",
    "                                                  feed_forward_dim, \n",
    "                                                  attention_dropout, \n",
    "                                                  ff_dropout, max_len) for _ in range(num_blocks)])\n",
    "        \n",
    "        self.projection_head = ProjectionHead(embedded_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask=None):\n",
    "        \n",
    "        sequence_len = input_ids.size(1)\n",
    "        assert sequence_len <= self.max_len, \"Sequence length exceeds the maximum length\"\n",
    "        \n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.positional_encoding(x)\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x, attention_mask)\n",
    "        \n",
    "        return self.projection_head(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257\n",
    "embedded_dim = 768\n",
    "max_len = 512\n",
    "embedding_dropout = 0.1\n",
    "num_blocks = 6\n",
    "num_heads = 8\n",
    "feed_forward_dim = 2048\n",
    "attention_dropout = 0.1\n",
    "ff_dropout = 0.1\n",
    "\n",
    "model = GPT(\n",
    "    vocab_size = vocab_size,\n",
    "    embedded_dim = embedded_dim,\n",
    "    max_len = max_len,\n",
    "    embedding_dropout = embedding_dropout,\n",
    "    num_blocks = num_blocks,\n",
    "    num_heads = num_heads,\n",
    "    feed_forward_dim = feed_forward_dim,\n",
    "    attention_dropout = attention_dropout,\n",
    "    ff_dropout = ff_dropout\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data = [\n",
    "    \"This is a sample sentence\",\n",
    "    \"This is another sample sentence\",\n",
    "    \"Let's change the sentence a bit\",\n",
    "    \"A more complex sentence with more words and more characters\"\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset): \n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_len\n",
    "        self.end_token = tokenizer.eos_token_id\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = self.data[idx]\n",
    "        input_ids = self.tokenizer(text, truncation=True, padding=False, return_tensors=\"pt\")[\"input_ids\"].squeeze()\n",
    "        \n",
    "        text_len = input_ids.size(0)\n",
    "        padding_len = max(self.max_length - text_len, 0)\n",
    "        padding = torch.full((padding_len,), self.end_token, dtype=torch.long)\n",
    "        \n",
    "        if text_len < self.max_length:\n",
    "            input_ids = torch.cat((input_ids, padding), dim=0)\n",
    "            label = torch.cat((input_ids[1:], padding[:-1] if padding_len > 0 else torch.tensor([self.end_token])), dim=0)\n",
    "        else:\n",
    "            input_ids = input_ids[:self.max_length]\n",
    "            label = torch.cat((input_ids[1:], torch.tensor([self.end_token])), dim=0)\n",
    "\n",
    "        return input_ids, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "train_dataset = CustomDataset(sample_data, tokenizer, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [388] at entry 0 and [391] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m      9\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     12\u001b[0m     input_ids, labels \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     13\u001b[0m     input_ids \u001b[38;5;241m=\u001b[39m input_ids\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/Documents/GitHub/DeepLearning_101/dl_101/lib/python3.9/site-packages/torch/utils/data/dataloader.py:631\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    629\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    630\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 631\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    632\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    633\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    634\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    635\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/Documents/GitHub/DeepLearning_101/dl_101/lib/python3.9/site-packages/torch/utils/data/dataloader.py:675\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    674\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 675\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    676\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    677\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/Documents/GitHub/DeepLearning_101/dl_101/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/DeepLearning_101/dl_101/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:277\u001b[0m, in \u001b[0;36mdefault_collate\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_collate\u001b[39m(batch):\n\u001b[1;32m    217\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;124;03m    Take in a batch of data and put the elements within the batch into a tensor with an additional outer dimension - batch size.\u001b[39;00m\n\u001b[1;32m    219\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;124;03m        >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[1;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault_collate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/GitHub/DeepLearning_101/dl_101/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[38;5;241m=\u001b[39mcollate_fn_map) \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/DeepLearning_101/dl_101/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:144\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    141\u001b[0m transposed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mbatch))  \u001b[38;5;66;03m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcollate\u001b[49m\u001b[43m(\u001b[49m\u001b[43msamples\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m samples \u001b[38;5;129;01min\u001b[39;00m transposed]  \u001b[38;5;66;03m# Backwards compatibility.\u001b[39;00m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    146\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/GitHub/DeepLearning_101/dl_101/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:121\u001b[0m, in \u001b[0;36mcollate\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m collate_fn_map \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m elem_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[0;32m--> 121\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcollate_fn_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43melem_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcollate_fn_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcollate_fn_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m collate_type \u001b[38;5;129;01min\u001b[39;00m collate_fn_map:\n\u001b[1;32m    124\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[0;32m~/Documents/GitHub/DeepLearning_101/dl_101/lib/python3.9/site-packages/torch/utils/data/_utils/collate.py:174\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[0;34m(batch, collate_fn_map)\u001b[0m\n\u001b[1;32m    172\u001b[0m     storage \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39m_typed_storage()\u001b[38;5;241m.\u001b[39m_new_shared(numel, device\u001b[38;5;241m=\u001b[39melem\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    173\u001b[0m     out \u001b[38;5;241m=\u001b[39m elem\u001b[38;5;241m.\u001b[39mnew(storage)\u001b[38;5;241m.\u001b[39mresize_(\u001b[38;5;28mlen\u001b[39m(batch), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mlist\u001b[39m(elem\u001b[38;5;241m.\u001b[39msize()))\n\u001b[0;32m--> 174\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [388] at entry 0 and [391] at entry 1"
     ]
    }
   ],
   "source": [
    "model.to(device)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 5\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs): \n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids, labels = batch\n",
    "        input_ids = input_ids.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        mask = generate_square_subsequent_mask(input_ids.size(1), device=device)\n",
    "        \n",
    "        \n",
    "        logits = model(input_ids, mask)\n",
    "        logits_flat = logits.view(-1, logits.size(-1))\n",
    "        labels_flat = labels.view(-1)\n",
    "        \n",
    "        loss = criterion(logits_flat, labels_flat)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        \n",
    "    print(f\"Epoch {epoch + 1} Loss: {total_loss / len(train_loader)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_101",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
